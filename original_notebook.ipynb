{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting User Preferences in Python using Alternating Least Squares\n",
    "\n",
    "In a previous [tutorial](http://online.cambridgecoding.com/notebooks/eWReNYcAfB/implementing-your-own-recommender-systems-in-python-2) you've learned about a powerful technique for automating item recommendations based on user's previous behaviour called **Collaborative Filtering** (CF). You implemented both the memory-based and the model-based method. For the memory-based method you used cosine similarity to calculate how similar users and/or items are. For the model-based method you utilised the fact that a matrix can be decomposed into a product of matrices, much like an integer can be factorised as a product of two integers (e.g., $10 = 2 \\times 5$):\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://s32.postimg.org/721lra91h/matrix.png\" style=\"max-width:100%; width: 50%\" alt=\"UV Decomposition\"/>\n",
    "\n",
    "In this tutorial you will learn in more detail how to solve the matrix factorisation problem shown in the figure above. The so-called $UV$-decomposition aims to approximately represent the rating matrix as a product of two matrices (usually called $U$ and $V$, hence the name). The challenge is to find the two underlying matrices, based on the few ratings you observed in the rating matrix. In the last [tutorial](http://online.cambridgecoding.com/notebooks/mhaller/implementing-your-own-recommender-systems-in-python-using-stochastic-gradient-descent-4#implementing-your-own-recommender-systems-in-python-using-stochastic-gradient-descent) you learned how to solve this problem using Stochastic Gradient Descent (SGD). Today you'll learn about another approach to finding the two matrices, called **Alternating Least Squares** (**ALS**). In this tutorial you will \n",
    "\n",
    "- gain some intuition about what ALS is,\n",
    "- derive the ALS algorithm,\n",
    "- train the model, and\n",
    "- make actual recommendations to users.\n",
    "\n",
    "## What is Alternating Least Squares (ALS)?\n",
    "Before strictly defining ALS, let's first think about the overall goal. The goal is to complete the rating matrix, i.e to accurately predict every user's rating for the movies they haven't watched yet. By predicting ratings accurately, you can suggest the most suitable movies to watch next to each user individually. Doing so will [keep the customers satisfied](https://www.youtube.com/watch?v=3YNsZLtzF18)!\n",
    "\n",
    "The rating matrix is your central data structure. When following the $UV$-decomposition approach, every rating can be considered a weighted linear combination of latent user and movie factors as shown in the figure below.\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://s32.postimg.org/4q9bmkfpx/matrix_mult.png\" style=\"max-width:100%; width: 50%\" alt=\"Matrix Multiplication\"/>\n",
    "\n",
    "Now recall how matrix multiplication works: To yield rating $r_{11}$ in the figure above, you have to take the dot product of the first row vector of $P^T$ and the first column vector of $Q$. You can interpret this operation as the sum of latent user features, weighted by the respective latent movie features. If you knew the latent movie features this would be a [simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression) problem. Since you don't know any of the latent features you'll use the trick of alternating least squares. **You assume to know the unknown movie features by randomly initialising the movie feature matrix $Q$**. You then estimate the user feature matrix for every user (row by row). After that you use this estimate of the user feature matrix to estiamte the movie feature matrix for every movie (column by column). If you repeat this procedure for every non-empty cell in the rating matrix a few times, you should get a pretty good approximation of the whole rating matrix.\n",
    "\n",
    "## Getting Ready\n",
    "Before deriving the algorithm let's prepare the development environment. Like the last time, let's import the [Movielens](http://files.grouplens.org/datasets/movielens/ml-100k.zip) data set and pre-process it by splitting the data into a training and test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 943 | Number of movies = 1682\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('./ml-100k/u.data', sep='\\t', names=header)\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "print 'Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "train_data, test_data = cv.train_test_split(df,test_size=0.25)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "\n",
    "# Create training and test matrix\n",
    "R = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    R[line[1]-1, line[2]-1] = line[3]  \n",
    "\n",
    "T = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    T[line[1]-1, line[2]-1] = line[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a \"selector\" matrix $I$ for your training data matrix $R$, which will contain $0$ if the rating matrix has no rating entry, and $1$ if the rating matrix contains an entry. This will come in handy later, when you want to select the appropriate rows and columns for estimating the parameters. Let's do the same for the test data matrix $T$ and call the selector matrix $I_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Index matrix for training data\n",
    "I = R.copy()\n",
    "I[I > 0] = 1\n",
    "I[I == 0] = 0\n",
    "\n",
    "# Index matrix for test data\n",
    "I2 = T.copy()\n",
    "I2[I2 > 0] = 1\n",
    "I2[I2 == 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of the model, you'll again use the root mean squared error (RMSE). Using the mean error rather than the sum of squared errors (SSD) has the benefit of making training and test error comparable due to normalisation. Since you only want to evaluate the performance based on observed ratings, select the appropriate rows and columns with the previously defined \"selector\" matrix $I$ and normalise the error by the number of ratings $R[R > 0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the RMSE\n",
    "def rmse(I,R,Q,P):\n",
    "    return np.sqrt(np.sum((I * (R - np.dot(P.T,Q)))**2)/len(R[R > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the ALS Algorithm\n",
    "Building upon the intuition from the beginning of the tutorial, let's define the model more thoroughly. Remember that you are trying to estimate two unknown matrices which, when multiplied together, yield the rating matrix. The loss function you will use is the well-known sum of squared errors. The second term is for regularisation to prevent overfitting (don't worry about it too much):\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\underset{Q*&space;,&space;P*}{min}\\sum_{(u,i)\\epsilon&space;K&space;}(r_{ui}-P_u^TQ_i)^2&plus;\\lambda(\\left&space;\\|&space;Q_i&space;\\right&space;\\|^2&space;&plus;&space;\\left&space;\\|&space;P_u&space;\\right&space;\\|^2)$&space;&space;$(1)\" title=\"\\underset{q* , p*}{min}\\sum_{(u,i)\\epsilon K }(r_{ui}-q_i^Tp_u)^2+\\lambda(\\left \\| q_i \\right \\|^2 + \\left \\| p_u \\right \\|^2)\" />\n",
    "\n",
    "\n",
    "Without the assumption of knowning one of the matrices $P$ or $Q$ up front, the problem is non-convex, i.e. it is not so easy to find a global optimum for your loss function. However, if you use the ALS trick to fix one of the matrices at each step, you can turn the non-convex optimisation problem into a quadratic (least squares) problem. Each step then boils down to solving an (overdetermined) system of linear equations. To find the actual update rules on how to calculate the rows of the matrices $P$ and $Q$ in each step, you have to find the global minimum of the loss function twice: once while considering $P$ to be a parameter and once while considering $Q$ to be a parameter. Luckliy you don't need to do the math, since [Zhou et al. (2008)](http://www.grappa.univ-lille3.fr/~mary/cours/stats/centrale/reco/paper/MatrixFactorizationALS.pdf) already did it in their paper \"*Large-scale Parallel Collaborative Filtering for the Netflix Prize*\" (with a slightly different regularization term). Differentiating the loss function w.r.t each parameter, equating to zero and solving for the respective paramter yields the following update equations:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\mathbf{p}_{i}&space;=&space;A^{-1}_{i}&space;V_{i}&space;\\&space;with\\&space;A_{i}&space;=&space;Q_{I_i}&space;Q_{I_i}^{T}&space;&plus;&space;\\lambda&space;n_{p_i}&space;E&space;\\&space;and\\&space;V_{i}&space;=&space;Q_{I_i}&space;R^{T}(i,I_{i})&space;\\&space;(2)\" title=\"\\mathbf{p}_{i} = A^{-1}_{i} V_{i} \\ with\\ A_{i} = Q_{I_i} Q_{I_i}^{T} + \\lambda n_{p_i} E \\ and\\ V_{i} = Q_{I_i} R^{T}(i,I_{i}) \\ (2)\" />\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\mathbf{q}_{j}&space;=&space;A^{-1}_{j}&space;V_{j}&space;\\&space;with\\&space;A_{j}&space;=&space;P_{I_j}&space;P_{I_j}^{T}&space;&plus;&space;\\lambda&space;n_{q_j}&space;E&space;\\&space;and\\&space;V_{j}&space;=&space;P_{I_j}&space;R^{T}(I_{j},j)&space;\\&space;(3)\" title=\"\\mathbf{q}_{j} = A^{-1}_{j} V_{j} \\ with\\ A_{j} = P_{I_j} P_{I_j}^{T} + \\lambda n_{q_j} E \\ and\\ V_{j} = P_{I_j} R^{T}(I_{j},j) \\ (3)\" />\n",
    "\n",
    "where $k$ is the number of latent features, $E$ is the (*k x k*)-dimensional identity matrix, $Q_{I_{i}}$ and $P_{I_{j}}$ are the sub-matrices of $Q$ and $P$ with the appropriate columns selected. $R^{T}(i,I_i)$ and $R^{T}(I_j,j)$ are row vectors with the appropriate rows and columns of $R$ selected. Finally $n_{p_{i}}$ denotes the number of items user $i$ has rated and $n_{q_{j}}$ denotes the number of users that rated item $j$. Using this result, the final algorithm is defined as follows:\n",
    "\n",
    "- Initialise matrix $P$ with average ratings for each movie in row ``1`` and random numbers otherwise\n",
    "- Repeat until convergence:\n",
    "    - Fix $Q$ and solve for $P$ according to equation ``(2)``\n",
    "    - Fix $P$ and solve for $Q$ according to equation ``(3)``\n",
    "    \n",
    "If you are eager to follow the derivation look up section 3.1. of [Zhou et al. (2008)](http://www.grappa.univ-lille3.fr/~mary/cours/stats/centrale/reco/paper/MatrixFactorizationALS.pdf). **The main take-away message is that after training, $p_i$ will contain the latent representation of the user $i$. Similarly $q_j$ will contain the latent representation of the movie $j$**. The approximation of the rating matrix is then calculated by organising the user and movie vectors in their respective matrices and multiplying those as shown in the first figure and equation ``(4)``.\n",
    "\n",
    "## Implementing the ALS Algorithm\n",
    "Let's start by defining the hyperparameters $\\lambda$ (regularisation weight) and $k$ (dimensionality of the latent feature space) and initialising the latent factor matrices $P$ and $Q$ as well as the identity matrix $E$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmbda = 0.1 # Regularisation weight\n",
    "k = 20 # Dimensionality of latent feature space\n",
    "m, n = R.shape # Number of users and items\n",
    "n_epochs = 15 # Number of epochs\n",
    "\n",
    "P = 3 * np.random.rand(k,m) # Latent user feature matrix\n",
    "Q = 3 * np.random.rand(k,n) # Latent movie feature matrix\n",
    "Q[0,:] = R[R != 0].mean(axis=0) # Avg. rating for each movie\n",
    "E = np.eye(k) # (k x k)-dimensional idendity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is implemented with one inner ``for``-loop for each parameter update step and an outer ``for``-loop to repeat this process until convergence. For simplicity, a fixed number of iterations (epochs) is defined and convergence is analysed later by plotting the learning curves on the training and test data set. Then it's only a matter of getting your update equations right and storing the training and test error for later analysis. Make sure you check for zero counts when setting $n_{p_{i}}$ and $n_{q_{j}}$. Zero counts would break the algorithm (by causing the matrices $A_{i}$ and $A_{j}$ to be singular, hence making the linear system unsolvable this way). Now grab a new cup of coffee, this might take a couple of minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/15] train error: 1.146596, test error: 1.263945\n",
      "[Epoch 2/15] train error: 0.863909, test error: 1.049361\n",
      "[Epoch 3/15] train error: 0.781007, test error: 1.008485\n",
      "[Epoch 4/15] train error: 0.741641, test error: 0.986250\n",
      "[Epoch 5/15] train error: 0.718968, test error: 0.972444\n",
      "[Epoch 6/15] train error: 0.704374, test error: 0.963087\n",
      "[Epoch 7/15] train error: 0.694283, test error: 0.956394\n",
      "[Epoch 8/15] train error: 0.686973, test error: 0.951439\n",
      "[Epoch 9/15] train error: 0.681503, test error: 0.947678\n",
      "[Epoch 10/15] train error: 0.677301, test error: 0.944766\n",
      "[Epoch 11/15] train error: 0.674000, test error: 0.942478\n",
      "[Epoch 12/15] train error: 0.671353, test error: 0.940655\n",
      "[Epoch 13/15] train error: 0.669197, test error: 0.939189\n",
      "[Epoch 14/15] train error: 0.667418, test error: 0.937998\n",
      "[Epoch 15/15] train error: 0.665936, test error: 0.937024\n"
     ]
    }
   ],
   "source": [
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Repeat until convergence\n",
    "for epoch in range(n_epochs):\n",
    "    # Fix Q and estimate P\n",
    "    for i, Ii in enumerate(I):\n",
    "        nui = np.count_nonzero(Ii) # Number of items user i has rated\n",
    "        if (nui == 0): nui = 1 # Be aware of zero counts!\n",
    "    \n",
    "        # Least squares solution\n",
    "        Ai = np.dot(Q, np.dot(np.diag(Ii), Q.T)) + lmbda * nui * E\n",
    "        Vi = np.dot(Q, np.dot(np.diag(Ii), R[i].T))\n",
    "        P[:,i] = np.linalg.solve(Ai,Vi)\n",
    "        \n",
    "    # Fix P and estimate Q\n",
    "    for j, Ij in enumerate(I.T):\n",
    "        nmj = np.count_nonzero(Ij) # Number of users that rated item j\n",
    "        if (nmj == 0): nmj = 1 # Be aware of zero counts!\n",
    "        \n",
    "        # Least squares solution\n",
    "        Aj = np.dot(P, np.dot(np.diag(Ij), P.T)) + lmbda * nmj * E\n",
    "        Vj = np.dot(P, np.dot(np.diag(Ij), R[:,j]))\n",
    "        Q[:,j] = np.linalg.solve(Aj,Vj)\n",
    "    \n",
    "    train_rmse = rmse(I,R,Q,P)\n",
    "    test_rmse = rmse(I2,T,Q,P)\n",
    "    train_errors.append(train_rmse)\n",
    "    test_errors.append(test_rmse)\n",
    "    \n",
    "    print \"[Epoch %d/%d] train error: %f, test error: %f\" \\\n",
    "    %(epoch+1, n_epochs, train_rmse, test_rmse)\n",
    "    \n",
    "print \"Algorithm converged\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After alternatingly training the parameters for ``15`` epochs, you can plot the learning curves to analyse the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEZCAYAAACXRVJOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10df58ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data');\n",
    "plt.plot(range(n_epochs), test_errors, marker='v', label='Test Data');\n",
    "plt.title('ALS-WR Learning Curve')\n",
    "plt.xlabel('Number of Epochs');\n",
    "plt.ylabel('RMSE');\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both training and test error monotonically decrease and converge over time. This means you don't seem to overfit (in which case the training error would still decrease, while the test error would increase).\n",
    "\n",
    "The model seems to perform quite well, with a relatively low RMSE after convergence. The performance can be influenced by tweaking the hyperparameters $\\lambda$ and $k$. In this case a manual grid search over $\\lambda$ led to the regularisation weight of ``0.1``, whilst a $k$ of ``20`` was just an initial choice. Zhou et al. found a weight of ``0.065`` and $k$ of ``20`` to yield the best performance on their data set. In the end it comes down to your task at hand, so just play around with these parameters and make sure you analyse your learning curves thoroughly.\n",
    "\n",
    "## Making Recommendations\n",
    "\n",
    "So far you have framed the recommendation problem in terms of a simple $UV$-decomposition, derived the algorithm and trained your model. Now, to see the model in action let's recommend five movies to the user ``17`` (arbitrary choice) based on the learned latent features. In order to do this you first calculate the prediction matrix <img src=\"https://latex.codecogs.com/gif.latex?\\hat{\\mathbf{r}}_{ui}=\\mathbf{p}_u^T\\mathbf{q}_{i}&space;\\&space;(4)\" title=\"\\hat{\\mathbf{r}}_{ui}=\\mathbf{p}_u^T\\mathbf{q}_{i} \\ (4)\" /> and convert it to a dataframe for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate prediction matrix R_hat (low-rank approximation for R)\n",
    "R_hat = pd.DataFrame(np.dot(P.T,Q))\n",
    "R = pd.DataFrame(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what you've achieved, let's compare some of your predictions for user ``17`` with the actual ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Rating</th>\n",
       "      <th>Predicted Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3.262819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>3.632967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>3.357371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>3.096711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>3.614416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual Rating  Predicted Rating\n",
       "0               4          3.262819\n",
       "6               4          3.632967\n",
       "8               3          3.357371\n",
       "12              3          3.096711\n",
       "99              4          3.614416"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare true ratings of user 17 with predictions\n",
    "ratings = pd.DataFrame(data=R.loc[16,R.loc[16,:] > 0]).head(n=5)\n",
    "ratings['Prediction'] = R_hat.loc[16,R.loc[16,:] > 0]\n",
    "ratings.columns = ['Actual Rating', 'Predicted Rating']\n",
    "\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model seems to have picked up the general trend of the user's ratings. To now give the top ``5`` recommendations to user ``17``, you simply want to find the five highest predicted movies that the user hasn't rated yet. To do that, use the original rating matrix as a logical index to pick out the appropriate predicted ratings, sort them in descending order and print out the top five recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>4.322413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>4.231591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.155992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>4.109113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>4.075898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Predicted Rating\n",
       "407          4.322413\n",
       "118          4.231591\n",
       "49           4.155992\n",
       "168          4.109113\n",
       "126          4.075898"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = R_hat.loc[16,R.loc[16,:] == 0] # Predictions for movies that the user 17 hasn't rated yet\n",
    "top5 = predictions.sort_values(ascending=False).head(n=5)\n",
    "recommendations = pd.DataFrame(data=top5)\n",
    "recommendations.columns = ['Predicted Rating']\n",
    "\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! Now fetch some meta data about the movies *407*, *118*, *49*, *168* and *126* from the database and display your recommendations to the user.\n",
    "\n",
    "## Summary\n",
    "After solving the matrix factorisation problem with SGD [last time](http://online.cambridgecoding.com/notebooks/mhaller/implementing-your-own-recommender-systems-in-python-using-stochastic-gradient-descent-4#implementing-your-own-recommender-systems-in-python-using-stochastic-gradient-descent), you've now learned another method to approximate the rating matrix called **Alternating Least Squares** (**ALS**). As you have seen, the ALS trick of alternatingly fixing half of the parameters lets you treat the problem as a simple linear regression problem. After estimating the user and movie matrix, you calculated your approximate rating matrix (prediction matrix) to pull out the top 5 recommendations for a specific user.\n",
    "\n",
    "Congratulations, you've mastered one of the fundamental ways of making recommendations. Stay tuned for future tutorials on how to employ more elaborate methods (e.g. hybrid recommender systems) to improve the performance of your models.\n",
    "\n",
    "## References\n",
    "- [Zhou et al. (2008)](http://www.grappa.univ-lille3.fr/~mary/cours/stats/centrale/reco/paper/MatrixFactorizationALS.pdf) Zhou, Y., Wilkinson, D., Schreiber, R. and Pan, R., 2008. Large-scale parallel collaborative filtering for the netflix prize. In Algorithmic Aspects in Information and Management (pp. 337-348). Springer Berlin Heidelberg.\n",
    "\n",
    "\n",
    "<hr>\n",
    "<strong>ABOUT THE AUTHORS</strong>\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://s31.postimg.org/km9i0wrp7/IMG_5951.jpg\" alt=\"Moritz Haller\"/>\n",
    "\n",
    "Moritz has spent the past years in industry, working on business intelligence applications with the company he co-founded. He holds a BSc in Computer Science and is currently pursuing an MSc in Computer Science at University College London. His main interests lie in probabilistic modelling and machine learning.\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"http://s31.postimg.org/lchvjpirf/Agnes_profile.png\" alt=\"Agnes Johannsdottir\"/>\n",
    "\n",
    "Agnes is a master student in Business Analytics at University College London. She studied Management Engineering in Iceland and worked for 2 years as an IT consultant in supply chain. Her main interests lie in using data science methods (especially machine learning) to apply in Retail and Supply Chain businesses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
